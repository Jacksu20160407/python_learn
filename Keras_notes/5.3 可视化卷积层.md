# Contents
### 5.3 可视化卷积神经网络

&emsp;&emsp;深度学习模型一般被认为是一个“黑盒子”，它们所学习到的特征表示很难用人的视角去查看。但是卷积网络并不是这样子的，卷积网络学习到的特征具有高度的可视化。从2013年开始就有许多关于可视化特征表示的技巧。我们现在介绍一个最常用的：

- 可视化中间卷积的输出。这能够让我们理解连续的卷积层对输入的转换，并且理解每一个卷积滤波器的意义。
- 可视化卷积滤波器。这对于准确理解卷积网络中每一个滤波器所接受到的视觉模式和概念非常有用（？）。
- 可视化图像中所需类别的热图。 这对于理解图像中哪一部分属于给定的目标类非常有用，并且能够用于定位目标。

##### 可视化中间卷积的输出
&emsp;&emsp;可视化中间卷积在于给网络一个输入，之后展示网络中卷积层和pooling层输出的各种特征映射。这展示的是网络如何将一个输入分解成各种不同的滤波器。这些特征映射有三个维度：宽，高，深度（通道数）。
每个通道都编码相互独立的特征，所以需要独立的画出每一个通道的内容，也就是2D图像。  
&emsp;&emsp;load模型：  
```python
from keras.models import load_model  
model = load_model('cats_and_dogs_small_2.h5')
model.summary()  # As a reminder.
```
&emsp;&emsp;输入一张图像：  
```python
img_path = '/Users/fchollet/Downloads/cats_and_dogs_small/test/cats/cat.1700.jpg'

# We preprocess the image into a 4D tensor
from keras.preprocessing import image
import numpy as np

img = image.load_img(img_path, target_size=(150, 150))
img_tensor = image.img_to_array(img)
img_tensor = np.expand_dims(img_tensor, axis=0)
# Remember that the model was trained on inputs
# that were preprocessed in the following way:
img_tensor /= 255.

# Its shape is (1, 150, 150, 3)
print(img_tensor.shape)
```
>(1, 150, 150, 3)
&emsp;&emsp;展示原图：
```python
import matplotlib.pyplot as plt
plt.imshow(img_tensor[0])
plt.show()
```
![下载.jpg](/Keras_notes/pics/下载.png)  

&emsp;&emsp;为了得到我们想要的特征映射图，先要创造一个模型并输入一张图像，之后在得到所有模型中所有卷积层和pooling层的输出激活值（每一层的输出经常成为激活值，因为它是激活函数的输出）。我们使用Keras的Model类，这个Model类有两个参数：一个输出tensor（或者一个包含多个输入tensor的list），另一个是输出tensor（或者是包含多个输出tensor的list）。实例化的结果是一个Keras模型，这与Sequential模型类似，将指定的输入映射到指定的输出。不同之处是Model允许多个输出，具体可以查看Keras文档。
```
from keras import models

# Extracts the outputs of the top 8 layers:
layer_outputs = [layer.output for layer in model.layers[:8]]
# Creates a model that will return these outputs, given the model input:
activation_model = models.Model(inputs=model.input, outputs=layer_outputs)
```
&emsp;&emsp;模型建立好了后，输入一张图片，模型就会返回每一层的激活值。到目前为止这可能是你第一次见到有多个输出的模型，一般情况下，一个模型可以有任意多个输入和输出。这个模型有1个输入8个输出，这8个输出就是网络模型每一层的激活值。
```
# This will return a list of 5 Numpy arrays:
# one array per layer activation
activations = activation_model.predict(img_tensor)
```
&emsp;&emsp;例如，这是我们输入一个猫得到的第一个卷积层的激活值。
```
first_layer_activation = activations[0]
print(first_layer_activation.shape)
```
> (1, 148, 148, 32)

&emsp;&emsp;这是一个148*148的特征映射，共有32个通道。看一下第三个通道**(实际中可能没有这个效果或者cmap='viridis'报错)**：
```
import matplotlib.pyplot as plt

plt.matshow(first_layer_activation[0, :, :, 3], cmap='viridis')
plt.show()
```
![channel_3.png](/Keras_notes/pics/channel_3.png)
&emsp;&emsp;可以看出这个通道是编码对角线边缘的。再看一下第30个通道--请注意，你的可能与这里的图像不同，因为卷积网络学习到的特定滤波器是不同的。
![channel_30.png](/Keras_notes/pics/channel_30.png)
&emsp;&emsp;这个可以看做检测明亮的绿点，能用来检测猫的眼睛。现在我们画出完整网络中所有激活值得可视化。我们将8个激活映射图的每一个通道都画出来，按通道堆叠成一个很大的图像。
```
import keras

# These are the names of the layers, so can have them as part of our plot
layer_names = []
for layer in model.layers[:8]:
    layer_names.append(layer.name)

images_per_row = 16

# Now let's display our feature maps
for layer_name, layer_activation in zip(layer_names, activations):
    # This is the number of features in the feature map
    n_features = layer_activation.shape[-1]

    # The feature map has shape (1, size, size, n_features)
    size = layer_activation.shape[1]

    # We will tile the activation channels in this matrix
    n_cols = n_features // images_per_row
    display_grid = np.zeros((size * n_cols, images_per_row * size))

    # We'll tile each filter into this big horizontal grid
    for col in range(n_cols):
        for row in range(images_per_row):
            channel_image = layer_activation[0,
                                             :, :,
                                             col * images_per_row + row]
            # Post-process the feature to make it visually palatable
            channel_image -= channel_image.mean()
            channel_image /= channel_image.std()
            channel_image *= 64
            channel_image += 128
            channel_image = np.clip(channel_image, 0, 255).astype('uint8')
            display_grid[col * size : (col + 1) * size,
                         row * size : (row + 1) * size] = channel_image

    # Display the grid
    scale = 1. / size
    plt.figure(figsize=(scale * display_grid.shape[1],
                        scale * display_grid.shape[0]))
    plt.title(layer_name)
    plt.grid(False)
    plt.imshow(display_grid, aspect='auto', cmap='viridis')
    
plt.show()
```
![conv2d_5.png](/Keras_notes/pics/conv2d_5.png)

![max_pooling2d_5.png](/Keras_notes/pics/max_pooling2d_5.png)

![conv2d_6.png](/Keras_notes/pics/conv2d_6.png)

![max_pooling2d_6.png](/Keras_notes/pics/max_pooling2d_6.png)

![conv2d_7.png](/Keras_notes/pics/conv2d_7.png)

![max_pooling2d_7.png](/Keras_notes/pics/max_pooling2d_7.png)

![conv2d_8.png](/Keras_notes/pics/conv2d_8.png)

![max_pooling2d_8.png](/Keras_notes/pics/max_pooling2d_8.png)
需要注意的几点：
- 第一层可以看成各种边缘检测器。在这个阶段，输出激活这保留了输入图像的绝大部分信息。
- 到深层网络以后，输出的激活值变得越来越抽象，视觉性越来越差。这是因为网络开始对高层概念，例如“猫耳朵”或者“猫眼”进行编码。高层的特征表示携带了越来越少的图像中可视化的内容，但是却增加了更多的关于图像类别的信息。
- 输出激活值的稀疏性随着网络深度的增加也在增加。第一层中所有的卷积滤波器都被激活了，但是接下来的几层中越来越多的滤波器变成了空白。这意味着该种滤波器的编码模式在输入图像中并没有发现。
&emsp;&emsp;通过以上过程可以发现，在深度卷积网络中，随着网络深度的增加，每一层提取的特征变得越来越抽象。越到深层，每层网络输出携带的可视化信息越来越少，关于目标类别的信息越来越多。一个深度神经网络可以看做一个信息提取管道，输入原始图像（例如RGB图像），经过重复转换（特征提取），与目标不相关的信息被滤掉（例如，图像的可视化信息），有用的信息被强化和提取（例如，图形的类别）。
这与人和动物感知世界的方式类似：观察一种场景几秒种后，我们能记住这个对象所呈现出来的抽象特征，但是却不能记住这些对象的细节表现。实际情况是，即使你一生中看了成千上万的自行车，你也不能记住它的所有细节。所以我们的大脑都是对输入的视觉信息进行抽象，变换成高层次的视觉概念并且滤掉不相关视觉细节，所以我们很难记住我们周围事物的详细样子。
##### 可视化卷积滤波器
&emsp;&emsp;为了查看卷积滤波器学习到的东西，我们将会展示每一个滤波器产生响应的视觉模型（就是展示使滤波器产生某种响应的输入，”响应“概念应该都懂的吧！）。这可以利用**输入空间的梯度上升**来完成：从空白输入图像开始，为了最大化卷积滤波器的响应，在卷积滤波器的输入图像上应用梯度下降。最后的结果就是对输入图像有最大响应的滤波器。
&emsp;&emsp;过程很简单，我们建造一个loss函数，这个loss函数能够最大化卷积层中滤波器的值，然后使用随机梯度下降来调节输入使输出激活值最大。例如，下面是VGG16网络结构中“block3_conv1”层中第0个滤波器输出值的loss
```
from keras.applications import VGG16
from keras import backend as K

model = VGG16(weights='imagenet',
              include_top=False)

layer_name = 'block3_conv1'
filter_index = 0

layer_output = model.get_layer(layer_name).output
loss = K.mean(layer_output[:, :, :, filter_index])
```
&emsp;&emsp;为了实现梯度下降，我们需要这个loss相对于模型输入的梯度。我们可以使用Keras中backend模块中的gradients函数包。
```
# The call to `gradients` returns a list of tensors (of size 1 in this case)
# hence we only keep the first element -- which is a tensor.
grads = K.gradients(loss, model.input)[0]
```
&emsp;&emsp;梯度下降过程中一个技巧就是除以梯度tensor的L2范数进行标准化。这样做是为了确保对输入图像的每次的更新都在相同的范围内。
```
# We add 1e-5 before dividing so as to avoid accidentally dividing by 0.
grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)
```
&emsp;&emsp;现在我们可以对输入的图像计算loss和Gradient tensor的值啦。可以定义一个iterate函数，这个函数输入一个Numpy tensor（也就是大小为1的tensor的list）输出一个包含两个Numpy tensors的list：loss和Gradient的值。
```
iterate = K.function([model.input], [loss, grads])

# Let's test it:
import numpy as np
loss_value, grads_value = iterate([np.zeros((1, 150, 150, 3))])
```
&emsp;&emsp;接下来定义一个Python 循环做梯度下降：
```
# We start from a gray image with some noise
input_img_data = np.random.random((1, 150, 150, 3)) * 20 + 128.

# Run gradient ascent for 40 steps
step = 1.  # this is the magnitude of each gradient update
for i in range(40):
    # Compute the loss value and gradient value
    loss_value, grads_value = iterate([input_img_data])
    # Here we adjust the input image in the direction that maximizes the loss
    input_img_data += grads_value * step
```
&emsp;&emsp;结果是一个浮点型性tensor，shape是（1, 150,150,3），数值范围是[0, 255]。所以我们需要一个后处理使这个tensor能有展示。下面函数实现这个功能：
```
def deprocess_image(x):
    # normalize tensor: center on 0., ensure std is 0.1
    x -= x.mean()
    x /= (x.std() + 1e-5)
    x *= 0.1

    # clip to [0, 1]
    x += 0.5
    x = np.clip(x, 0, 1)

    # convert to RGB array
    x *= 255
    x = np.clip(x, 0, 255).astype('uint8')
    return x
```
&emsp;&emsp;现在我们有了所有的切片（原文片段），下面定义一个函数，这个函数的输入是一层的名字，滤波器的索引，返回值是一个图像tensor，这个tensor表示最大化指定滤波器的激活值的图像模式。
```
def generate_pattern(layer_name, filter_index, size=150):
    # Build a loss function that maximizes the activation
    # of the nth filter of the layer considered.
    layer_output = model.get_layer(layer_name).output
    loss = K.mean(layer_output[:, :, :, filter_index])

    # Compute the gradient of the input picture wrt this loss
    grads = K.gradients(loss, model.input)[0]

    # Normalization trick: we normalize the gradient
    grads /= (K.sqrt(K.mean(K.square(grads))) + 1e-5)

    # This function returns the loss and grads given the input picture
    iterate = K.function([model.input], [loss, grads])
    
    # We start from a gray image with some noise
    input_img_data = np.random.random((1, size, size, 3)) * 20 + 128.

    # Run gradient ascent for 40 steps
    step = 1.
    for i in range(40):
        loss_value, grads_value = iterate([input_img_data])
        input_img_data += grads_value * step
        
    img = input_img_data[0]
    return deprocess_image(img)
```
&emsp;&emsp;展示如下：
```
plt.imshow(generate_pattern('block3_conv1', 0))
plt.show()
```
![gradient_pock.png](/Keras_notes/pics/gradient_pock.png)  

&emsp;&emsp;可以看出block3_conv1中的滤波器0响应的是波尔卡圆点模式（polka dot pattern）。
现在我们能够可视化每一层中的每一个滤波器。为简单起见，我们只可视化每一个卷积块（block1_conv1, block2_conv1, block3_conv1, block4_conv1, block5_conv1）中的第一个层中的前64个滤波器。我们将输出排列成一个8*8的网格，每个网格是64*64的滤波器模式，每个滤波器之间有一些黑色的边框。
```
for layer_name in ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1']:
    size = 64
    margin = 5

    # This a empty (black) image where we will store our results.
    results = np.zeros((8 * size + 7 * margin, 8 * size + 7 * margin, 3))

    for i in range(8):  # iterate over the rows of our results grid
        for j in range(8):  # iterate over the columns of our results grid
            # Generate the pattern for filter `i + (j * 8)` in `layer_name`
            filter_img = generate_pattern(layer_name, i + (j * 8), size=size)

            # Put the result in the square `(i, j)` of the results grid
            horizontal_start = i * size + i * margin
            horizontal_end = horizontal_start + size
            vertical_start = j * size + j * margin
            vertical_end = vertical_start + size
            results[horizontal_start: horizontal_end, vertical_start: vertical_end, :] = filter_img

    # Display the results grid
    plt.figure(figsize=(20, 20))
    plt.imshow(results)
    plt.show()
```
![filter_1.png](/Keras_notes/pics/filter_1.png)
![filter_2.png](/Keras_notes/pics/filter_2.png)
![filter_3.png](/Keras_notes/pics/filter_3.png)
![filter_4.png](/Keras_notes/pics/filter_4.png)  

&emsp;&emsp;这些滤波器告诉我们卷积网络层是如何看这个世界的：在一个卷积神经网络中，每一层都简单地学习了一组滤波器，这样它们的输入就可以被作为滤波器的组合来表达。这类似于傅里叶变换将信号分解成cos函数的形式。当我们进入网络的深层时，这些卷积滤波器变得越来越复杂和精炼。
- 模型中的第一层（block1_conv1）滤波器编码（学习）的是一些简单的带有方向的边缘和颜色（或者是有颜色的边缘）。
- 从block2_conv1开始滤波器编码一些简单的纹理，这些纹理是有边缘和颜色组成的。
- 更高层的滤波器开始与自然图像中发现的纹理相似：比如羽毛，眼睛，树叶等。
##### 可视化激活类的热图
&emsp;&emsp;接下来接受另外一个可视化的技巧，这个技巧能让我们理解输入图像中的哪一部分导致卷积网络作出最后的类别决定（也就是对类别归属起决定作用的部分）。这在调试卷积网络过程中非常有用，尤其是分类错误的时候。同时这个技巧也可以用于定位。  

&emsp;&emsp;这个技巧通常称为“类激活映射（CAM）”可视化，它是在输入图像上产生一个类激活的热图。类激活热图是一个与特定输出类相关的2D网格图，它计算的是输入图像的每一个位置与输出类相关联的重要性（也就是属于这个类别的概率）。例如，我们输入猫狗分类卷积网络一张图像，CAM就能够让我们产生一个关于猫这一类的热图，这个热图展示了图像的每一部分与猫的相似性，同样的，对于狗这个类别，这个热图也展示了每一部分与狗的相似性。  

&emsp;&emsp;关于具体实现请参考[here](https://arxiv.org/abs/1610.02391)，过程很简单：首先获得一张输入图像在卷积层的输出特征映射，之后衡量特征映射中每一个通道的重要性，衡量的方式是每一类对通道的梯度值（？）。更直观的说，理解这种技巧的方式是，根据“每个通道对类的重要性”，对“输入图像如何激活不同通道的强度”的空间映射进行加权，最终产生了一个“输入图像激活类的强烈程度”的空间映射（貌似更加难以理解啦！）。
还是看例子吧！
```
from keras.applications.vgg16 import VGG16

K.clear_session()

# Note that we are including the densely-connected classifier on top;
# all previous times, we were discarding it.
model = VGG16(weights='imagenet')
```
&emsp;&emsp;输入图像是两只在散步的非洲象：
![elephants.png](/Keras_notes/pisc/elephants.png)
&emsp;&emsp;把图像转变成VGG16模型能够读取的模式：图像大小应该为224*224，并且使用keras.applications.vgg16.preprocess_input进行预处理。代码如下：
```
from keras.preprocessing import image
from keras.applications.vgg16 import preprocess_input, decode_predictions
import numpy as np

# The local path to our target image
img_path = '/Users/fchollet/Downloads/creative_commons_elephant.jpg'

# `img` is a PIL image of size 224x224
img = image.load_img(img_path, target_size=(224, 224))

# `x` is a float32 Numpy array of shape (224, 224, 3)
x = image.img_to_array(img)

# We add a dimension to transform our array into a "batch"
# of size (1, 224, 224, 3)
x = np.expand_dims(x, axis=0)

# Finally we preprocess the batch
# (this does channel-wise color normalization)
x = preprocess_input(x)

preds = model.predict(x)
print('Predicted:', decode_predictions(preds, top=3)[0])
```
>Predicted: [('n02504458', 'African_elephant', 0.90942144), ('n01871265', 'tusker', 0.08618243), ('n02504013', 'Indian_elephant', 0.0043545929)]

&emsp;&emsp;前三个预测分数值最高的分别是：
- 非洲象（92.5%的概率）
- 有长牙的动物（象，野猪等）（7%的概率）
- 印度象（0.4%的概率）
&emsp;&emsp;所以我们的网络已经识别出这是一张包含不确定数目非洲象的图像。在预测向量中被激活程度最大的是与“非洲象”类别相对应的条目，这个条目的索引是386。
```
np.argmax(preds[0])
```
>386

&emsp;&emsp;为了可视化输入图像中哪一部分是与“非洲象”最相似的部分，我们建立一个Grad-CAM过程：
```
# This is the "african elephant" entry in the prediction vector
african_elephant_output = model.output[:, 386]

# The is the output feature map of the `block5_conv3` layer,
# the last convolutional layer in VGG16
last_conv_layer = model.get_layer('block5_conv3')

# This is the gradient of the "african elephant" class with regard to
# the output feature map of `block5_conv3`
grads = K.gradients(african_elephant_output, last_conv_layer.output)[0]

# This is a vector of shape (512,), where each entry
# is the mean intensity of the gradient over a specific feature map channel
pooled_grads = K.mean(grads, axis=(0, 1, 2))

# This function allows us to access the values of the quantities we just defined:
# `pooled_grads` and the output feature map of `block5_conv3`,
# given a sample image
iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])

# These are the values of these two quantities, as Numpy arrays,
# given our sample image of two elephants
pooled_grads_value, conv_layer_output_value = iterate([x])

# We multiply each channel in the feature map array
# by "how important this channel is" with regard to the elephant class
for i in range(512):
    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]

# The channel-wise mean of the resulting feature map
# is our heatmap of class activation
heatmap = np.mean(conv_layer_output_value, axis=-1)
```
&emsp;&emsp;为了查看方便，将热图normalize到0-1之间：
```
heatmap = np.maximum(heatmap, 0)
heatmap /= np.max(heatmap)
plt.matshow(heatmap)
plt.show()
```
![normalize_heat_map.png](/Keras_notes/pics/normalize_heat_map.png)
&emsp;&emsp;将原图和热图进行叠加后产生一幅新的图像：
```
import cv2

# We use cv2 to load the original image
img = cv2.imread(img_path)

# We resize the heatmap to have the same size as the original image
heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))

# We convert the heatmap to RGB
heatmap = np.uint8(255 * heatmap)

# We apply the heatmap to the original image
heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

# 0.4 here is a heatmap intensity factor
superimposed_img = heatmap * 0.4 + img

# Save the image to disk
cv2.imwrite('/Users/fchollet/Downloads/elephant_cam.jpg', superimposed_img)
```
![elephants_add_heat_map.jpg](/Keras_notes/pics/elephants_add_heat_map.jpg)
&emsp;&emsp;这个可视化技巧回答了两个重要的问题：
- 为什么这个网络结构认为图像中包含非洲象？
- 包含的非洲象在图像的什么位置？
尤其需要注意的是，图中小象的耳朵被强烈的激活：这可能是网络结构认为非洲象和印度象的重要区别。
